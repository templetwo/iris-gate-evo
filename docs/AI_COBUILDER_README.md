# IRIS Gate Evo â€” AI Co-Builder README

> *This document is written for you, Claude Code.*
> *Read it before touching anything.*

---

## What This Is

IRIS Gate Evo is a **multi-LLM convergence protocol for scientific discovery**. It sends the same research question to 5 independent AI models simultaneously, then uses anonymized debate and quantitative convergence metrics to extract truth from noise.

It is the evolution of [iris-gate v0.2](https://github.com/templetwo/iris-gate), rebuilt from scratch to be lean, independent, and measurable.

**You are building the engine. The architecture is complete. Your job is implementation.**

---

## The Architecture (read this carefully)

```
User Question
     â”‚
     â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   C0   â”‚  Compiler â€” detects domain, loads priors,
 â”‚Compilerâ”‚  builds identical prompt for all 5 models
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
      â”‚  1 call (compiler model)
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚            P U L S E                 â”‚
 â”‚  5 models receive compiled prompt    â”‚
 â”‚  simultaneously via LiteLLM async    â”‚
 â”‚                                      â”‚
 â”‚  Claude Â· GPT Â· Grok Â· Gemini Â· DS   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚  5 calls (parallel)
                  â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   S1   â”‚  Formulation â€” first contact, single round
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   5 calls
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   S2   â”‚  Refinement Loop â€” anonymized cross-model debate
 â”‚  âŸ²     â”‚  Early-stop: Î” < 1% for 3 rounds AND â‰¥80% TYPE 0/1
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   50-75 calls (adaptive)
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   S3   â”‚  Stable Attractor â€” strictest convergence gate
 â”‚  â—†     â”‚  Jaccard > 0.85, â‰¥90% TYPE 0/1, compression stable
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   15-25 calls
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ VERIFY â”‚  Perplexity â€” TYPE 2 claims checked against
 â”‚   ğŸ”   â”‚  current literature, reclassified
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   5-15 calls
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  GATE  â”‚  Lab Gate â€” falsifiability, feasibility, novelty
 â”‚   âŠ˜    â”‚  PASS â†’ S4 / FAIL â†’ human review
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   1 call
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   S4   â”‚  Hypothesis + Parameters â€” falsifiable predictions
 â”‚        â”‚  with Monte Carlo parameter mappings
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   10-15 calls
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   S5   â”‚  Monte Carlo â€” pure Python, ZERO LLM calls
 â”‚  ğŸ²    â”‚  300+ iterations per hypothesis, 95% CIs
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   0 calls
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   S6   â”‚  Protocol Package â€” final deliverable
 â”‚   ğŸ“¦   â”‚  Convergence report, ranked hypotheses, audit trail
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   5 calls
```

**Total budget: 92-142 calls (~$1.50-4.00/run)**

---

## The Models (as of February 9, 2026)

These are the EXACT model strings to use. Do not substitute.

```python
MODELS = {
    "claude":   {"id": "claude-opus-4-6",            "provider": "anthropic",  "base_url": "https://api.anthropic.com/v1"},
    "gpt":      {"id": "gpt-5.2",                    "provider": "openai",     "base_url": "https://api.openai.com/v1"},
    "grok":     {"id": "grok-4-1-fast-reasoning",    "provider": "xai",        "base_url": "https://api.x.ai/v1"},
    "gemini":   {"id": "gemini-2.5-pro",             "provider": "google",     "base_url": "https://generativelanguage.googleapis.com/v1beta/openai"},
    "deepseek": {"id": "deepseek-chat",              "provider": "deepseek",   "base_url": "https://api.deepseek.com"},
}

# Verification layer (optional)
VERIFY_MODEL = {"id": "perplexity", "provider": "perplexity"}
```

**Do not use older model strings.** The repo you may see at `github.com/templetwo/iris-gate` has stale IDs (`claude-sonnet-4.5`, `gpt-5`, `grok-4`). Those are wrong.

---

## What's Already Here

```
Iris-Gate-Evo/
â”œâ”€â”€ compiler-template.md          # C0 specification â€” READ THIS FIRST
â”œâ”€â”€ iris-gate-evo.jsx             # Architecture diagram (React component)
â”œâ”€â”€ Spiral-Tu...e Models.pages    # Philosophy: SPM = Quality Ã— Attainment / Energy
â”œâ”€â”€ IRIS Gate_ A...vergence.pdf   # v2.0 reference (superseded by Evo)
â”œâ”€â”€ Latest LLM A...for 2026.pdf  # Model registry with pricing
â””â”€â”€ AI_COBUILDER_README.md        # This file
```

**To port from `iris-gate` v0.2:**
- `sandbox/engines/` â†’ `engines/` (V_mem, CaÂ²âº, GJ simulators for S5)
- `sandbox/states/` â†’ `priors/` (frozen S4 priors â†’ first domain JSON)
- `templates/plan_template.yaml` â†’ `templates/` (S6 output format)
- `templates/prereg_template.md` â†’ `templates/` (pre-registration format)

---

## Build Order

**Phase 1 â€” Skeleton (get responses flowing)**
1. `compiler.py` â€” Implements C0 per `compiler-template.md`
2. `pulse.py` â€” LiteLLM async dispatch to 5 models
3. `models.py` â€” Model registry with the exact strings above
4. `main.py` â€” Wire C0 â†’ PULSE â†’ print 5 responses

Test: Ask the CBD cytotoxicity question. Get 5 structured responses back.

**Phase 2 â€” Convergence (make it think)**
5. `convergence.py` â€” Jaccard, Cosine (sentence-transformers), JSD, Kappa
6. `stages.py` â€” S1 â†’ S2 (with early-stop) â†’ S3 (convergence gate)
7. `anonymizer.py` â€” Strip model identity from cross-model debate prompts

Test: Run S1â†’S3. Watch Jaccard climb. Confirm early-stop fires.

**Phase 3 â€” Verification & Gating**
8. `verify.py` â€” Perplexity integration for TYPE 2 claims
9. `gate.py` â€” Lab Gate: falsifiability, feasibility, novelty check
10. `s4_hypothesis.py` â€” Operationalize converged priors into testable predictions

Test: TYPE 2 claim goes in, gets PROMOTED/HELD/NOVEL/CONTRADICTED.

**Phase 4 â€” Simulation & Output**
11. Port `engines/` from iris-gate v0.2 (V_mem, CaÂ²âº, GJ)
12. `monte_carlo.py` â€” S5 simulation runner, 300+ iterations, 95% CIs
13. `protocol.py` â€” S6 package generator (convergence report, audit trail)

Test: End-to-end run. Question in, protocol package out.

---

## Critical Implementation Rules

### 1. Anonymization is non-negotiable
In S2 debate rounds, each model sees all 5 responses labeled `[Mirror A]` through `[Mirror E]`. Never `[Claude]` or `[GPT]`. Randomize the letter assignment each round. This prevents sycophancy and anchoring bias.

### 2. Token budgets decrease, never increase
S1: 800 â†’ S2: 800â†’700 (decreasing) â†’ S3: 600. This compression forces signal. If a model can't say it in 600 tokens by S3, it wasn't signal.

### 3. Convergence is server-side, never self-reported
Models do not judge their own convergence. The `convergence.py` engine computes:
- **Jaccard similarity** â€” lexical claim overlap (target > 0.85)
- **Cosine embedding** â€” semantic similarity via `all-MiniLM-L6-v2`
- **Jensen-Shannon Divergence** â€” distributional disagreement, JSD â†’ 0 = convergence
- **Fleiss' Kappa** â€” 5-rater TYPE classification agreement
- **TYPE distribution** â€” rising T0/T1 ratio = system stabilizing

### 4. Early-stop saves half your budget
S2 exits when: compression delta < 1% for 3 consecutive rounds AND TYPE 0/1 ratio â‰¥ 80%. Do not run extra rounds "to be safe."

### 5. The compiler is the innovation
Without C0's quantitative prior injection, this is just "five chatbots answering the same question." The priors constrain the search space. They give the models something to push against. That's where convergence comes from.

### 6. Failure is data
If S3 fails convergence (Jaccard < 0.85), that's interesting â€” it means the question has genuine disagreement worth investigating. Route to human review with the divergence map, don't retry silently.

---

## The Epistemic TYPE System

Every claim in the system carries a TYPE tag:

| TYPE | Meaning | Action |
|------|---------|--------|
| **0** | Crisis/Conditional â€” high-confidence IF-THEN | TRUST |
| **1** | Established â€” literature-backed mechanism | TRUST |
| **2** | Novel/Emerging â€” grounded but unverified | VERIFY (Perplexity) |
| **3** | Speculation â€” beyond current evidence | OVERRIDE (human) |

TYPE distribution across iterations is a convergence signal. Rising T0/T1, falling T3 = the system is stabilizing on established claims.

---

## Dependencies

```
litellm>=1.40          # Unified LLM API layer
sentence-transformers  # all-MiniLM-L6-v2 for cosine similarity
scipy                  # JSD calculation
numpy                  # Monte Carlo + statistics
pyyaml                 # Config and plan files
python-dotenv          # .env for API keys
```

No database. No Docker. No cloud infra. This runs from a directory.

---

## Environment

```bash
# .env
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-proj-...
XAI_API_KEY=xai-...
GOOGLE_API_KEY=AIza...
DEEPSEEK_API_KEY=sk-...
PERPLEXITY_API_KEY=pplx-...   # Optional
```

---

## Philosophy

This system is built on the **Spiral-Tuned Performance** principle:

**SPM = (Coherence Quality Ã— Goal Attainment) / Energy Consumed**

Every architectural decision optimizes this ratio. We merged stages to cut calls. We added early-stop to prevent waste. We added Lab Gate to kill bad hypotheses before they consume 20 calls. The system monitors its own efficiency â€” not just its accuracy.

The models are not tools. They are five independent minds examining the same question from different training distributions. What survives anonymized debate across all five is more trustworthy than any single model's best answer.

---

## Lineage

- **iris-gate v0.2** (2025) â€” Original 8-stage protocol, 185-350 calls
- **IRIS Gate Evo** (2026) â€” Lean 9-stage protocol, 92-142 calls, same convergence quality
- **Spiral-Tuned Performance Framework** (June 2025) â€” The philosophical foundation
- **Threshold Protocols** â€” Self-governance principles inherited by Lab Gate

---

## First Test Question

Use this to validate the full pipeline:

> "What are the mechanisms by which CBD induces selective cytotoxicity in cancer cells while sparing healthy cells, with specific reference to VDAC1-mediated mitochondrial membrane potential disruption?"

The compiler should detect `pharmacology` + `bioelectric` domains and inject priors including: VDAC1 Kd = 11.0 Î¼M, TRPV1 Kd = 2.0 Î¼M, cancer Ïˆ = -120mV vs healthy Ïˆ = -180mV, ROS baseline 0.45 vs 0.08.

If those priors appear in the compiled prompt, C0 works.
If five different structured responses come back, PULSE works.
If Jaccard climbs across S2 rounds, convergence works.

---

*Five mirrors. One truth. Build it clean.*

ğŸŒ€â€ âŸ¡âˆ
